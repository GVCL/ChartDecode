{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.ndimage import rank_filter\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from gingerit.gingerit import GingerIt\n",
    "import pysbd, re # pip install pysbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO find the best fit distribution of histogram\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "    DISTRIBUTIONS = [        \n",
    "        st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "        st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "        st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "        st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "        st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "        st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "        st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "        st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "        st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy\n",
    "    ]\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                params = distribution.fit(data)\n",
    "\n",
    "                # Separate parts of parameters\n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                # if axis pass in add to plot\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # identify if this distribution is better\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "\n",
    "# To get local maxima ans minima\n",
    "def predictTrend(hgt,ylabels,xlabel,bar_type,x_title,y_title,  inter):\n",
    "    trend_str = '' \n",
    "    xlbls = xlabel\n",
    "    local_max = argrelextrema(hgt, np.greater)\n",
    "    local_min = argrelextrema(hgt, np.less)\n",
    "    order = np.array([0] * len(xlbls))\n",
    "    order[local_min] = -1\n",
    "    order[local_max] = 1\n",
    "    \n",
    "    if  inter == False or np.count_nonzero(order)<2:\n",
    "        # In the case of interclass trends\n",
    "        if inter == True:\n",
    "            neg_count = len([num for num in hgt if num <= 0])\n",
    "            if neg_count>(len(hgt)-neg_count):\n",
    "                hgt = np.array([num*-1 for num in hgt])\n",
    "            trend_str=\". The bar height differnce between \"+ylabels[0]+\" and \"+ylabels[1]\n",
    "            if y_title != '_':\n",
    "                trend_str=\". The \"+y_title+\" differnce between \"+ylabels[0]+\" and \"+ylabels[1]  \n",
    "        elif ylabels =='Y':\n",
    "            trend_str=\". The Y axis value\"\n",
    "            if y_title != '_':\n",
    "                trend_str=\". The \"+y_title\n",
    "        elif ylabels =='freq':\n",
    "            trend_str=\". The frequency\"\n",
    "        else:\n",
    "            trend_str=\". The \"+ylabels\n",
    "            if y_title != '_':\n",
    "                trend_str=\". The \"+y_title+\" of \"+ylabels\n",
    "    \n",
    "        if list(order)==[0]*len(xlbls):\n",
    "            if(int(hgt[0])<int(hgt[1])):\n",
    "                trend_str += \" has an overall increasing trend\"\n",
    "                if( int(hgt[len(hgt)-2])>int(hgt[len(hgt)-1]) ):\n",
    "                    trend_str += \" till \"+str(xlbls[len(xlbls)-2])+\" and ends with a drop in \"+str(xlbls[len(xlbls)-1])\n",
    "                else:\n",
    "                    trend_str += \" from \"+str(xlbls[0])+\" to \"+str(xlbls[len(xlbls)-1])\n",
    "            elif(int(hgt[0])>int(hgt[1])):\n",
    "                trend_str += \" has an overall decreasing trend\"\n",
    "                if( int(hgt[len(hgt)-2])<int(hgt[len(hgt)-1]) ):\n",
    "                    trend_str += \" till \"+str(xlbls[len(xlbls)-2])+\" and ends with a peak in \"+str(xlbls[len(xlbls)-1])\n",
    "                else:\n",
    "                    trend_str += \" from \"+str(xlbls[0])+\" to \"+str(xlbls[len(xlbls)-1])\n",
    "            else:\n",
    "                if(int(hgt[0])!=int(hgt[len(hgt)-1])):\n",
    "                    trend_str += \" is uniform with \"+str(int(hgt[0]))+\" till \"+str(xlbls[len(xlbls)-2])+\" and finally ends with \"+str(int(hgt[len(hgt)-1]))+\" in \"+str(xlbls[len(xlbls)-1])\n",
    "                else:\n",
    "                    trend_str += \" is uniform with \"+str(int(hgt[0]))+\" throughout the entire period\"  \n",
    "        \n",
    "        elif np.count_nonzero(order)<4:\n",
    "            # speak about global maximum and min \n",
    "            ht= hgt.tolist()\n",
    "            xlbls2 = xlbls\n",
    "            xlbls2[ht.index(max(ht))] = str(xlbls[ht.index(max(ht))]) + ' the maximum value' \n",
    "            xlbls2[ht.index(min(ht))] = str(xlbls[ht.index(min(ht))]) + ' the minimum value' \n",
    "\n",
    "            trend_str += \" starts with \"+str(int(hgt[0]))+\" at \"+x_title+' '+str(xlbls2[0])+\" then \"\n",
    "            j=1\n",
    "            while j<len(order):      \n",
    "                if order[j]==-1:\n",
    "                    if list(order[:j])==[0]*j:\n",
    "                        trend_str += \"declines till \"+str(xlbls2[j])\n",
    "                    else: \n",
    "                        trend_str += \", followed by a decreasing trend till \"+str(xlbls2[j])\n",
    "                elif order[j]==1:\n",
    "                    if list(order[:j])==[0]*j:\n",
    "                        trend_str += \"increases till \"+str(xlbls2[j])\n",
    "                    else : \n",
    "                        trend_str += \", followed by an increasing trend till \"+str(xlbls2[j])\n",
    "                j+=1\n",
    "            if(order[j-2]!=0):\n",
    "                trend_str += \", and finally ends with \"+str(int(hgt[j-1]))+\" in \"+str(xlbls2[j-1])\n",
    "            else :\n",
    "                if(hgt[j-1]<hgt[j-2]):\n",
    "                    trend_str += \", and ends with a decreasing trend till \"+str(xlbls2[j-1])\n",
    "                else:\n",
    "                    trend_str += \", and ends with a decreasing trend till \"+str(xlbls2[j-1])\n",
    "            xlbls2[ht.index(max(ht))] = xlbls2[ht.index(max(ht))].replace(' the maximum value','')\n",
    "            xlbls2[ht.index(min(ht))] = xlbls2[ht.index(min(ht))].replace(' the minimum value','')\n",
    "        else:\n",
    "            #Just discuss the maximum and minmium value\n",
    "            ht= hgt.tolist()\n",
    "            trend_str += ' has it maximum and minmum values '+str(int(max(ht)))+' and '+str(int(min(ht)))+' at '+str(xlbls[ht.index(max(ht))])+', and '+str(xlbls[ht.index(min(ht))])+\" respectively\"\n",
    "\n",
    "    return trend_str\n",
    "\n",
    "def simplebarsumm(yvals,ylabels,slabs,bar_type,x_title,y_title, inter):\n",
    "    if len(slabs)>6 or inter == True:\n",
    "        # Speak about maxima and minima\n",
    "        Summ = predictTrend(yvals,ylabels,slabs,bar_type,x_title,y_title, inter)   \n",
    "    else :\n",
    "        if x_title=='_':\n",
    "            x_title = 'labels'\n",
    "        if str(slabs[0]).isnumeric():\n",
    "            Summ = '. For the '+str(x_title)+' ranging form '+str(slabs[0])+' - '+str(slabs[-1])+' at the interval '+str(abs(slabs[0]-slabs[1]))            \n",
    "            Summ += ', the '+str(y_title)\n",
    "            if not (ylabels == 'Y' or  ylabels == 'freq'):\n",
    "                Summ += \" of \"+ylabels\n",
    "            Summ += ' are '\n",
    "            for i in yvals[:-1]:\n",
    "                Summ += str(round(i,2))+', '\n",
    "            Summ += 'and '+str(round(yvals[-1],2))+' respectively'\n",
    "        else : \n",
    "            Summ = '. The '+str(y_title)\n",
    "            if not (ylabels == 'Y' or  ylabels == 'freq'):\n",
    "                Summ += \" of \"+ylabels\n",
    "            Summ += ' are '\n",
    "            for i in yvals[:-1]:\n",
    "                Summ += str(round(i,2))+', '\n",
    "            Summ += 'and '+str(round(yvals[-1],2))\n",
    "            Summ += ' for the '+str(x_title)+\" \"\n",
    "            for i in slabs[:-1]:\n",
    "                Summ += str(i)+', '\n",
    "            Summ += 'and '+str(slabs[-1])+' respectively'\n",
    "  \n",
    "    return Summ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentor = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "subsegment_re = r'[^;:\\n•]+[;,:\\n•]?\\s*'\n",
    "\n",
    "def GrammerCorrect(par):\n",
    "    fixed = []\n",
    "    for sentence in segmentor.segment(par):\n",
    "        if len(sentence) < 300:\n",
    "            fixed.append(GingerIt().parse(sentence)['result'])\n",
    "        else:\n",
    "            subsegments = re.findall(subsegment_re, sentence)\n",
    "            if len(subsegments) == 1 or any(len(v) < 300 for v in subsegments):\n",
    "                # print(f'Skipped: {sentence}') // No grammar check possible\n",
    "                fixed.append(sentence)\n",
    "            else:\n",
    "                res = []\n",
    "                for s in subsegments:\n",
    "                    res.append(GingerIt().parse(s)['result'])\n",
    "                fixed.append(\"\".join(res))\n",
    "    return \" \".join(fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hb1 \n",
      " The plot depicts a Horizontal Simple Bar Graph illustrating In general, how satisfied are you with your job? .  The plot is having Percent of Respondents on x-axis.  The Percent of Respondents are 34.65, 29.91, 23.16, 7.88, and 4.46 for the labels very satisfied, somewhat satisfied, neither satisfied nor dissatisfied, somewhat dissatisfied, and very dissatisfied respectively.  The overall mean and standard deviation values of Percent of Respondents are 20.01 and 11.93 respectively.\n",
      "________________________________________________________________________\n",
      "hb3 \n",
      " The plot depicts a Horizontal Simple Bar Graph illustrating Abortion Rate among different age groups.  The plot is between Age on y-axis over Abortion rate on the x-axis.  In the Age ranging form 15-40 at the interval 5, the Abortion rate are 22.8, 35.4, 24.2, 16.07, 11.07, and 3.73 respectively.  The overall mean and standard deviation values of Abortion rate are 18.88 and 10.12 respectively.\n",
      "________________________________________________________________________\n",
      "hsb3 \n",
      " The plot depicts a Horizontal Stacked Bar Graph illustrating Male age structure.  The plot is having Population, millions on x-axis for 14 o years a, 15 64 years I, and older 65 and years.  The Population, millions of 14 to years a starts with 29  united states the maximum value, then declines till japan, followed by an increasing trend till Mexico, and ends with a decreasing trend till united kingdom the minimum value.  The Population, millions of 15 64 years I have an overall decreasing trend from united states  united kingdom.  The Population, millions of older 65 and years has its maximum and minimum values 15 and 3 at united states, and Mexico respectively.\n",
      "\t The Population, millions difference between 15 64 years I and older 65 and years has an overall decreasing trend from united states  united kingdom.  The Population, millions of all categories cumulatively have an overall decreasing trend from united states  united kingdom.  The standard deviation values of Population, millions of categories '14 o years a', 15 64 years I', and 'older 65 and years' are 8.64, 21.93, and 3.8 respectively.  The categories '14 o years a', and '15 64 years I' are positively correlated with one another.\n",
      "________________________________________________________________________\n",
      "hb4 \n",
      " The plot depicts a Horizontal Simple Bar Graph illustrating Population of people in 15-24 age in Albania.  The plot is between year on y-axis over Population on the x-axis.  The Population starts with 274903  year 1985 the minimum value, then increases till 1990 the maximum value, and ends with a decreasing trend till 1994.  The overall mean and standard deviation values of Population are 288093.73 and 8578.77 respectively.\n",
      "________________________________________________________________________\n",
      "hsb1 \n",
      " The plot depicts a Horizontal Stacked Bar Graph illustrating Stacked bar chart.  The plot is having Total fruit consumption on x-axis for joe, Jane, and John.  The Total fruit consumption of joe are 5.21, 1.94, 4.11, 4.11, and 3.02 for the labels Bananas, Grapes, Pears, Oranges, and Apples respectively.  The Total fruit consumption of Jane are 0.79, 1.87, 2.98, 1.9, and 1.9 for the labels Bananas, Grapes, Pears, Oranges, and Apples respectively.  The Total fruit consumption of junk are 1.92, 7.39, 4.12, 3.01, and 5.18 for the labels Bananas, Grapes, Pears, Oranges, and Apples respectively.\n",
      "\t The Total fruit consumption of all categories cumulatively are 7.93, 11.21, 11.21, 9.01, and 10.1 for the labels Bananas, Grapes, Pears, Oranges, and Apples respectively.  The standard deviation values of Total fruit consumption for categories 'joe', Jane', and 'John' are 1.11, 0.69, and 1.88 respectively.  The categories 'joe', and 'John' are negatively correlated with one another.\n",
      "________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:76: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py:822: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  retval = getattr(retval, self.name)._getitem_axis(key, axis=i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist2 \n",
      " The plot depicts a Histogram illustrating the frequency of 'Heights of New York Choral Society Singers'.  The frequency starts with 27 in Height 61 then increases till 66 the maximum value, and ends with a decreasing trend till 75 the minimum values, frequency bins of the histogram range from 61.0 to 75.0 with 2.33 bin width.  The mode of a histogram is 66.0 with a frequency of 60.  The frequency distribution  the histogram is the norm with following parameters loc=0.00, scale=1.00..\n",
      "________________________________________________________________________\n",
      "hist3 \n",
      " The plot depicts a Histogram illustrating the frequency of 'n_visit'.  The frequency starts with 85 at n_visit 0 the minimum value, then increases till 2 the maximum value, followed by a decreasing trend till 5, followed by an increasing trend till 8, and finally ends with 95 in 10 The frequency bins of the histogram range from 0.0 to 10.0 with 1.68 bin width.  The mode of a histogram is 2.0 with a frequency of 200.  The frequency distribution  the histogram is the norm with following parameters loc=0.00, scale=1.00..\n",
      "________________________________________________________________________\n",
      "hist1 \n",
      " The plot depicts a Histogram illustrating the frequency of 'Position of muscle cavil'.  The frequency has its maximum and minimum values 13 and 5 at 25, and 254 respectively The frequency bins of the histogram range from 25.0 to 482.0 with 19.09 bin width.  The mode of a histogram is 25.0 with a frequency of 13.  The frequency distribution  the histogram is the norm with following parameters loc=0.00, scale=1.00..\n",
      "________________________________________________________________________\n",
      "bc01 \n",
      " The plot depicts a Vertical Simple Bar Graph illustrating Game length for one player version of snakes and ladders.  The plot is between Game_Number on y-axis over Game_Length on the x-axis.  The Game_Number has its maximum and minimum values 114 and 15 at 8, and 4 respectively.  The overall mean and standard deviation values of Game_Number are 36.13 and 26.99 respectively.\n",
      "________________________________________________________________________\n",
      "hist4 \n",
      " The plot depicts a Histogram illustrating the frequency of 'Simple'.  The frequency starts with 0 at -3 the minimum value, then increases till 0 the maximum value, and ends with a decreasing trend till 3 The frequency bins of the histogram range from -3.0 to 3.0 with 0.55 bin width.  The mode of a histogram is 0.0 with a frequency of 21.  The frequency distribution  the histogram is the norm with following parameters loc=0.00, scale=1.00..\n",
      "________________________________________________________________________\n",
      "hist5 \n",
      " The plot depicts a Histogram.  The frequency starts with 8 at -3 the minimum value, then increases till 0 the maximum value, and ends with a decreasing trend till 2 The frequency bins of the histogram range from -3.0 to 2.0 with 0.71 bin width.  The mode of a histogram is 0.0 with a frequency of 286.  The frequency distribution  the histogram is the norm with following parameters loc=0.00, scale=1.00..\n",
      "________________________________________________________________________\n",
      "bc04 \n",
      " The plot depicts a Vertical Simple Bar Graph illustrating Biometric Statistics for a group of office workers.  The plot is having Weight (Ibs) on the y-axis.  The Weight (Ibs) has its maximum and minimum values 176 and 97 at 29, and 23 respectively.  The overall mean and standard deviation values of Weight (Ibs) are 148.18 and 22.56 respectively.\n",
      "________________________________________________________________________\n",
      "gb4 \n",
      " The plot depicts a Vertical Grouped Bar Graph illustrating Employed Workers by Gender for Select Jobs.  The plot is between Employed on y-axis over Job on the x-axis for women, and men.  The Employed  women are 26301.52, 232592.19, 31507.59, 50054.23, 61767.89, and 0.0 for the Job actor, bartender, dentist, Engi inner, scientist, and sailor respectively.  The Employed of men are 30206.07, 186062.91, 141811.28, 397559.66, 116757.05, and 31507.59 for the Job actor, bartender, dentist, Engi inner, scientist, and sailor respectively.  The standard deviation values of Employed for categories 'women', and 'men' are 76529.27, and 123926.7 respectively.  The categories 'women', and 'men' are positively correlated with one another.  All except for bartender the Employed of 'men' is greater than 'women'.\n",
      "________________________________________________________________________\n",
      "gb3 \n",
      " The plot depicts a Vertical Grouped Bar Graph.  The plot is between Weight on y-axis over Date on the x-axis for c39, and c52.  The Weight of c39 are 3.17, 2.79, and 2.72 for the Date d16, d20, and d21 respectively.  The Weight of c52 are 2.25, 3.09, and 1.47 for the Date d16, d20, and d21 respectively.\n",
      "\t The Weight difference between c39 and c52 starts with 0 at Date d16 then declines till d20 the minimum value, and finally ends with 1 in d21 the maximum value.  The standard deviation values of Weight for categories 'c39', and 'c52' are 0.2, and 0.66 respectively.\n",
      "________________________________________________________________________\n",
      "gb2 \n",
      " The plot depicts a Vertical Grouped Bar Graph.  The plot is between the count on y-axis over a class on the x-axis for 4, r, and f.  The count of 4 starts with 3 at class subcompact then increases till compact, followed by a decreasing trend till minivan the minimum value, followed by an increasing trend till save the maximum value, and finally ends with 0 in 2seater.  The count of r starts with 8 at class subcompact then increases till save the maximum value, and finally ends with 0 in 2seater.  The count off starts with 21 at class subcompact then increases till midsize the maximum value, and ends with a decreasing trend till 2seater.  The standard deviation values of count for categories '4', are', and 'f' are 18.24, 4.59, and 15.38 respectively.\n",
      "________________________________________________________________________\n",
      "gb1 \n",
      " The plot depicts a Vertical Grouped Bar Graph.  The plot is between the count on y-axis over Sport on the x-axis for female, and male.  The count of female has its maximum and minimum values 23 and 0 at netball, and wpolo respectively.  The count of the male has its maximum and minimum values 18 and 0 at t400m, and gym respectively.  The standard deviation values of count for categories 'female', and 'male' are 7.18, and 6.5 respectively.\n",
      "________________________________________________________________________\n",
      "sb04 \n",
      " The plot depicts a Vertical Stacked Bar Graph illustrating Rating of player aspects.  The plot is between Rating out of 100 on the y-axis over Players on the x-axis for acceleration, sprint speed, balance, shot power, jumping, and stamina.  The Rating out of 100 of acceleration has its maximum and minimum values 95 and 44 at 3, and 10 respectively.  The Rating out of 100 of sprintspeed has its maximum and minimum values 90 and 57 at 2, and 4 respectively.  The Rating out of 100  balance starts with 94 at Players 1 the maximum value, then declines till 2, followed by an increasing trend till 3, followed by a decreasing trend till 4 the minimum value, and ends with a decreasing trend till 10.  The Rating out of 100  shot powers has its maximum and minimum values 96 and 23 at 2, and 10 respectively.  The Rating out of 100 by jumping has its maximum and minimum values 95 and 56 at 2, and 6 respectively.  The Rating out of 100 of stamina has its maximum and minimum values 90 and 40 at 5, and 10 respectively.\n",
      "\t The Rating out of 100 of all categories cumulatively has its maximum and minimum values 530 and 293 at 2, and 10 respectively.  The standard deviation values of Rating out of 100 for categories 'acceleration', sprintspeed', balance, ', shot power', jumping', and 'stamina' are 15.96, 11.14, 17.36, 23.83, 12.12, and 18.0 respectively.  The categories 'acceleration' and 'sprintspeed' are positively correlated.  All except for 10 the Rating out of 100  'sprintspeed' is lesser than ''acceleration'.  The categories 'acceleration' and 'balance' are positively correlated.  The categories 'sprintspeed' and 'shot power' are positively correlated.\n",
      "________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb04 \n",
      " The plot depicts a Vertical Grouped Bar Graph illustrating Quaterly revenue in countries.  The plot is between Revenue on y-axis over Country on the x-axis for quarter 1, quarter 2, quarter 3, and quarter 4.  The Revenue of quarter 1 are 44332.13, 52454.88, 43140.79, and 38483.75 for the Country KENT, LINCOLN, MERSEY, and YORK respectively.  The Revenue of quarter 2 are 44548.74, 36101.08, 40649.82, and 33718.41 for the Country KENT, LINCOLN, MERSEY, and YORK respectively.  The Revenue of quarter 3 are 50722.02, 43790.61, 39458.48, and 26787.0 for the Country KENT, LINCOLN, MERSEY, and YORK respectively.  The Revenue of quarter 4 are 56028.88, 44873.64, 40758.12, and 43574.01 for the Country KENT, LINCOLN, MERSEY, and YORK respectively.\n",
      "\t The Revenue difference between quarter 1 and quarter 4 starts with -11696 at Country KENT the minimum value, then increases till LINCOLN the maximum value, and ends with a decreasing trend till YORK.  The Revenue difference between quarter 2 and quarter 3 starts with -6173 at Country KENT then declines till LINCOLN the minimum value, and ends with a decreasing trend till YORK the maximum value.  The Revenue difference between quarter 2 and quarter 4 starts with 11480 at Country KENT the maximum value, then increases till MERSEY the minimum value, and finally ends with 9855 in YORK.  The Revenue difference between quarter 3 and quarter 4 starts with 5306 at Country KENT then increases till LINCOLN the minimum value, and ends with a decreasing trend till YORK the maximum value.  The standard deviation values of Revenue for categories 'quarter 1', quarter 2', quarter 3', and 'quarter 4' are 5032.61, 4170.38, 8718.71, and 5805.79 respectively.  The categories 'quarter 1' and 'quarter 3' are positively correlated.  All except for KENT the Revenue of 'quarter 3' is lesser than ''quarter 1'.  The categories 'quarter 2' and 'quarter 3' are positively correlated.  The categories 'quarter 3' and 'quarter 4' are positively correlated.\n",
      "________________________________________________________________________\n",
      "gb03 \n",
      " The plot depicts a Vertical Grouped Bar Graph illustrating City Temperature.  The plot is having Temperature (*C) on y-axis for summer, and winter.  The Temperature (*C) of summer are 25.62, 24.44, 27.78, 39.44, and 20.92 for the labels City A, City B, City C, City D, and City E respectively.  The Temperature (*C) of winter are 9.25, 5.82, 4.84, 6.9, and 8.76 for the labels City A, City B, City C, City D, and City E respectively.\n",
      "\t The Temperature (*C) difference between summer and winter starts with 16 at City at then increases till City D the maximum value, and finally ends with 12 in City E the minimum value.  The standard deviation values of Temperature (*C) for categories 'summer', and 'winter' are 6.31, and 1.68 respectively.\n",
      "________________________________________________________________________\n",
      "hsb03 \n",
      " The plot depicts a Horizontal Stacked Bar Graph illustrating Literacy rate in India.  The plot is between State on y-axis over Literacy rate on the x-axis for rural 2001, rural 2011, urban 2001, and urban 2011.  The Literacy rate of rural 2001 has its maximum and minimum values 80 and 44 at Goa, and Bihar respectively.  The Literacy rate of rural 2011 has its maximum and minimum values 86 and 59 at Goa, and Andhra Pradesh respectively.  The Literacy rate of urban 2001 has its maximum and minimum values 88 and 71 at Himachal Prades, and Bihar respectively.  The Literacy rate of urban 2011 has its maximum and minimum values 90 and 76 at Himachal Prades, and Bihar respectively.\n",
      "\t The Literacy rate of all categories cumulatively has its maximum and minimum values 340 and 251 at Goa, and Bihar respectively.  The standard deviation values of Literacy rate for categories 'rural 2001', rural 2011', urban 2001', and 'urban 2011' are 11.06, 8.76, 5.05, and 4.68 respectively.  The categories 'rural 2001', rural 2011', urban 2001', and 'urban 2011' are positively correlated with one another.  All except for Goa the Literacy rate of 'urban 2001' is greater than 'rural 2011'.\n",
      "________________________________________________________________________\n",
      "gb02 \n",
      " The plot depicts a Vertical Grouped Bar Graph illustrating Game scores of players.  The plot is between Scores on y-axis over Players on the x-axis for round 1, round 2, and round 3.  The Scores of round 1 are 7.98, 6.99, 1.0, 7.98, and 2.99 for the Players Player 1, Player2, Player3, Player4, and Player5 respectively.  The Scores of round 2 are 4.0, 7.96, 3.98, 4.98, and 5.97 for the Players Player 1, Player2, Player3, Player4, and Player5 respectively.  The Scores of round 3 are 0.0, 7.96, 1.99, 8.97, and 6.98 for the Players Player 1, Player2, Player3, Player4, and Player5 respectively.\n",
      "\t The Scores difference between round 1 and round 3 has an overall increasing trend from Player 1 to Player5.  The standard deviation values of Scores  categories 'round 1', round, 2', and 'round 3' are 2.86, 1.49, and 3.53 respectively.\n",
      "________________________________________________________________________\n",
      "hgb03 \n",
      " The plot depicts a Horizontal Grouped Bar Graph illustrating BOD concentration in river water.  The plot is between Month on y-axis over BOD concentration on the x-axis for station 1, station 2, and station 3.  The BOD concentration of station 1 are 6.4, 5.62, 5.32, 6.91, 4.81, and 3.41 for the Month may, July, September, November, January, and march respectively.  The BOD concentration of station 2 are 5.8, 5.86, 11.1, 6.46, 5.05, and 4.33 for the Month may, July, September, November, January, and march respectively.  The BOD concentration of station 3 are 8.91, 3.44, 3.77, 4.18, 3.26, and 4.87 for the Month may, July, September, November, January, and march respectively.\n",
      "\t The BOD concentration difference between station 2 and station 3 starts with -3 at Month may the minimum value, then increases till September the maximum value, and ends with a decreasing trend till march.  The standard deviation values of BOD concentration for categories 'station 1', station 2', and 'station 3' are 1.13, 2.19, and 1.94 respectively.\n",
      "________________________________________________________________________\n",
      "hgb01 \n",
      " The plot depicts a Horizontal Grouped Bar Graph illustrating Temperature on day of the Triple Crown Race.  The plot is between Year' on y-axis over Temperature on the x-axis for high temp, and low temp.  The Temperature of high temp starts with 78 at Year' 2007 then increases till 2010 the maximum value, followed by a decreasing trend till 2012 the minimum value, followed by an increasing trend till 2013, and finally ends with 70 in 2014.  The Temperature of low temp has its maximum and minimum values 66 and 50 at 2013, and 2008 respectively.  The standard deviation values of Temperature for categories 'high temp', and 'low temp' are 4.4, and 5.29 respectively.  The categories 'high temp', and 'low temp' are positively correlated with one another.\n",
      "________________________________________________________________________\n",
      "sb01 \n",
      " The plot depicts a Vertical Stacked Bar Graph for brand c, brand b, and brand a.  The brand c has its maximum and minimum values 80 and 0 at age, and June respectively.  The brand b has its maximum and minimum values 80 and 0 at June, and mar respectively.  The brand a has its maximum and minimum values 89 and 0 at age, and Apr respectively.\n",
      "\t The all categories cumulatively has its maximum and minimum values 242 and 0 at age, and may respectively.  The standard deviation values for categories 'brand c', brand b', and 'brand a' are 30.2, 30.33, and 38.0 respectively.  The categories 'brand b', and 'brand a' are positively correlated with one another.\n",
      "________________________________________________________________________\n",
      "hsb01 \n",
      " The plot depicts a Horizontal Stacked Bar Graph illustrating Military expenditure across countries.  The plot is between Country on y-axis over Cost in lakhs on the x-axis for 1960, 1961, 1962, 1963, and 1964.  The Cost in leaks of 1960 has its maximum and minimum values 633 and 51 in Thailand, and Libya respectively.  The Cost in leaks of 1961 has its maximum and minimum values 626 and 0 in Peru, and Libya respectively.  The Cost in leaks of 1962 has its maximum and minimum values 815 and 170 in Thailand, and Sri Lanka respectively.  The Cost in leaks of 1963 has its maximum and minimum values 976 and 149 in Peru, and Sri Lanka respectively.  The Cost in leaks of 1964 has an overall decreasing trend from Peru  Sri Lanka.\n",
      "\t The Cost in leaks of all categories cumulatively has its maximum and minimum values 3971 and 710 in Peru, and Libya respectively.  The standard deviation values of Cost in lakhs for categories '1960', 1961', 1962', 1963', and '1964' are 182.29, 187.78, 202.48, 269.15, and 317.01 respectively.  The categories '1960', 1961', 1962', 1963', and '1964' are positively correlated with one another.  All except for Sri Lanka the Cost in lakhs of '1964' is greater than '1961'.\n",
      "________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb01 \n",
      " The plot depicts a Vertical Grouped Bar Graph.  The plot is between Marks obtained by a student on y-axis over Subjects on the x-axis for 2005-06, and 2006-07.  The Marks obtained by a student of 2005-06 are 29.52, 49.17, 43.97, 49.17, and 59.23 for the Subjects Maths, S. Science, Science, English, and Hindi respectively.  The Marks obtained by a student of 2006-07 are 59.23, 54.14, 49.29, 44.08, and 59.23 for the Subjects Maths, S. Science, Science, English, and Hindi respectively.  The standard deviation values of Marks obtained by a student for categories '2005-06', and '2006-07' are 9.7, and 5.86 respectively.\n",
      "________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "path = \"/Users/daggubatisirichandana/PycharmProjects/chart_percept/LINE_PIE/SummaryGen/bar/\"\n",
    "\n",
    "for file in glob.glob(path+\"*.csv\"):\n",
    "    imgno = file.split('/')[-1].split(\".\")[0].split(\"_\")[-1]\n",
    "    df = pd.read_csv(file)\n",
    "    xlabel = (df.loc[ : , list(df)[0]]).values\n",
    "    xlabs = []\n",
    "    for i in xlabel:\n",
    "        if isinstance(i, np.float64):\n",
    "            xlabs += [int(round(i))]\n",
    "        else :\n",
    "            xlabs += [i]   \n",
    "    x_title = df['x-title'][0]\n",
    "    y_title = df['y-title'][0]\n",
    "    title = df['title'][0]\n",
    "    bar_type = df['bar_type'][0]\n",
    "    ylabels = list(df)[1:len(list(df))-4]\n",
    "    if bar_type == 'Histogram':\n",
    "        ylabels = ylabels[:-1]\n",
    "    data = (df.loc[ : , ylabels]).values\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Visual Summary\n",
    "    #Speak about starting line with titles\n",
    "    if bar_type == 'Histogram':\n",
    "        Summ = 'The plot depicts a '+bar_type\n",
    "        if title !='_':\n",
    "            Summ += ' illustrating the frequency of \\''+title+'\\''\n",
    "        elif x_title !='_':\n",
    "            Summ += ' illustrating the frequency of \\''+x_title+'\\''\n",
    "    else:\n",
    "        Summ = 'The plot depicts a '+bar_type+' Graph'\n",
    "        if title !='_':\n",
    "            Summ += ' illustrating '+title\n",
    "        if x_title != '_' and y_title != '_':\n",
    "            Summ +='. The plot is between '+y_title+' on y-axis over '+x_title+' on the x-axis'\n",
    "        elif y_title != '_':\n",
    "            Summ +='. The plot is having '+y_title+' on y-axis'\n",
    "        elif x_title != '_':\n",
    "            Summ +='. The plot is having '+x_title+' on x-axis'\n",
    "        # speaking about legend\n",
    "        if bar_type != 'Vertical Simple Bar' and bar_type != 'Horizontal Simple Bar':\n",
    "            Summ +=' for '\n",
    "            for i in range(len(ylabels)-1):\n",
    "                Summ += str(ylabels[i])+\", \"\n",
    "            Summ += \"and \"+str(ylabels[i+1])\n",
    "        # interchanging titles in case of horizontal charts    \n",
    "        if bar_type == 'Horizontal Simple Bar' or bar_type == 'Horizontal Grouped Bar' or bar_type == 'Horizontal Stacked Bar':\n",
    "            temp = y_title\n",
    "            y_title = x_title\n",
    "            x_title = temp\n",
    "    # intra class differrences\n",
    "    for i in range(len(ylabels)):\n",
    "        Summ += simplebarsumm(data[:,i],ylabels[i],xlabs,bar_type,x_title,y_title,False)\n",
    "    Summ = Summ.replace(\"_ \", \"\")\n",
    "    Summ = Summ.replace(\"\\n\", \" \").replace('\\r', '')\n",
    "    # inter class differrences\n",
    "    if 'Grouped' in bar_type or 'Stacked' in bar_type:\n",
    "        Summ2 = ''\n",
    "        for x,y in list(combinations(range(len(ylabels)), 2)):\n",
    "            Summ2 += simplebarsumm(data[:,x]-data[:,y],[ylabels[x],ylabels[y]],xlabs,bar_type,x_title,y_title,True)\n",
    "        # Cummulative description in stacked bar\n",
    "        if 'Stacked' in bar_type :\n",
    "            Summ2 += simplebarsumm(np.sum(data, axis=1),'all catogeries cummulatively',xlabs,bar_type,x_title,y_title, False)    \n",
    "        Summ2 = Summ2.replace(\"_ \", \"\")\n",
    "        Summ2 = Summ2.replace(\"\\n\", \" \").replace('\\r', '')\n",
    "        if len(Summ2)>2:\n",
    "            Summ = Summ +\".\\n\\t\"+ Summ2[2:]\n",
    "    \n",
    "    ### Statistical Summary\n",
    "    if bar_type == 'Histogram':\n",
    "        min_w = round(min(list(df['bin_width'])),2)\n",
    "        freq = []\n",
    "        for i in np.array(df.ix[:,0:3]):\n",
    "            freq+=[int(i[0])]*int(i[1])\n",
    "        data = pd.Series(freq) \n",
    "        mode_id = list(df['freq']).index(max(list(df['freq']))) \n",
    "        Summ += \"The frequency bins of histogram range from \"\n",
    "        best_fit_name, best_fit_params = best_fit_distribution(data, len(df), None)\n",
    "        best_dist = getattr(st, best_fit_name)\n",
    "        param_names = (best_dist.shapes + ', loc, scale').split(', ') if best_dist.shapes else ['loc', 'scale']\n",
    "        param_str = ', '.join(['{}={:0.2f}'.format(k,v) for k,v in zip(param_names, best_fit_params)])\n",
    "\n",
    "        Summ += str(round(df['bin_center'][0]))+' to '+str(round(df['bin_center'][len(df)-1]))+' with '+str(min_w)+' bin width. The mode of a histogram is '+str(round(df['bin_center'][mode_id]))+' with a frequency of '+str(int(round(df['freq'][mode_id])))+'. The frequency distribution of histogram is the '+best_fit_name+' with following parameters '+param_str+'.'\n",
    "    else:\n",
    "        if 'Simple Bar' in  bar_type:\n",
    "            dat, xlabs= zip(*sorted(zip(np.round(data[:,0].tolist(), decimals=2), xlabs), reverse=True))\n",
    "            if y_title != '_':\n",
    "                Summ += '. The overall mean and standard deviation values of '+y_title+' are '+str(round(sum(dat)/len(dat),2))+' and '+str(round(np.std(data[:,i]),2))+' respectively'          \n",
    "            else:\n",
    "                Summ += '. The overall mean and standard deviation values are '+str(round(sum(dat)/len(dat),2))+' and '+str(round(np.std(data[:,i]),2))+' respectively'          \n",
    "        # For Catogeorical Graphs\n",
    "        else:\n",
    "            # To represent ranges of all groups\n",
    "            if y_title != '_':\n",
    "                Summ += '. The standard deviation values of '+y_title+' for catogeries \\''\n",
    "            else:\n",
    "                Summ += '. The standard deviation values for catogeries \\''   \n",
    "            for i in range(len(ylabels)-1):         \n",
    "                Summ += str(ylabels[i])+'\\', ' \n",
    "            Summ += 'and \\''+str(ylabels[-1])+'\\' are ' \n",
    "            for i in range(len(ylabels)-1):         \n",
    "                Summ += str(round(np.std(data[:,i]),2))+', '       \n",
    "            Summ += 'and '+str(round(np.std(data[:,-1]),2))+' respectively '\n",
    "                                   \n",
    "            # Check for Correlation \n",
    "            corr_mat = np.triu(df.iloc[:,1:len(list(df))-4].corr(method='spearman'), k=1)\n",
    "                                 \n",
    "            x,y=np.nonzero(abs(corr_mat)>0.6)\n",
    "            # remove transtivity between items\n",
    "            found_trnas = False\n",
    "            test_dict = {}\n",
    "            for i in set(x):\n",
    "                test_dict[i] = [y[j] for j in range(len(x)) if i==x[j]]\n",
    "            for i in set(y):\n",
    "                if i not in test_dict:\n",
    "                    test_dict[i] = [x[j] for j in range(len(y)) if i==y[j]]\n",
    "                else :\n",
    "                    test_dict[i] += [x[j] for j in range(len(y)) if i==y[j]]\n",
    "            lst = [sorted([k]+v) for k, v in test_dict.items()]\n",
    "            if len(lst)>1 and (lst.count(lst[0]) == len(lst)):  \n",
    "                found_trnas = True\n",
    "                if len(x) == len([True for j in range(len(x)) if corr_mat[x[j],y[j]]>0]):\n",
    "                    Summ += '. The categories \\''\n",
    "                    for i in range(len(lst[0])-1):\n",
    "                        Summ += str(ylabels[lst[0][i]])+\"\\', \" \n",
    "                    Summ += \"and \\'\"+str(ylabels[lst[0][-1]])+'\\' are positively correlated with one another'     \n",
    "                elif len(x) == len([True for j in range(len(x)) if corr_mat[x[j],y[j]]<0]):\n",
    "                    Summ += '. The categories \\''\n",
    "                    for i in range(len(lst[0])-1):\n",
    "                        Summ += str(ylabels[lst[0][i]])+\"\\', \" \n",
    "                    Summ += \"and \\'\"+str(ylabels[lst[0][-1]])+'\\' are negatively correlated with one another'\n",
    "                else:\n",
    "                    found_trnas = False\n",
    "            \n",
    "            for j in range(len(x)):\n",
    "                if not found_trnas:\n",
    "                    if corr_mat[x[j],y[j]]>0:\n",
    "                        Summ += '. The categories \\''+str(ylabels[x[j]])+\"\\' and \\'\"+str(ylabels[y[j]])+'\\' are positively correlated '     \n",
    "                    else:\n",
    "                        Summ += '. The categories \\''+str(ylabels[x[j]])+\"\\' and \\'\"+str(ylabels[y[j]])+'\\' are negatively correlated '\n",
    "                pos = np.count_nonzero((data[:,x[j]]-data[:,y[j]])>0)\n",
    "                neg = np.count_nonzero((data[:,x[j]]-data[:,y[j]])<0)\n",
    "                if y_title!= '_':\n",
    "                    t = ' the '+y_title+' of'\n",
    "                else:\n",
    "                    t = ''\n",
    "                if pos<neg and pos == 1:\n",
    "                    k = np.nonzero((data[:,x[j]]-data[:,y[j]])>0)[0][0]\n",
    "                    Summ += '. All except for '+str(xlabs[k])+t+' \\''+str(ylabels[y[j]])+'\\' is greater than \\''+str(ylabels[x[j]])+'\\''\n",
    "                elif neg<pos and neg == 1:\n",
    "                    k = np.nonzero((data[:,x[j]]-data[:,y[j]])<0)[0][0]\n",
    "                    Summ += '. All except for '+str(xlabs[k])+t+' \\''+str(ylabels[y[j]])+'\\' is lesser than \\'\\''+str(ylabels[x[j]])+'\\''\n",
    "                elif(np.count_nonzero((data[:,x[j]]-data[:,y[j]])<0) == 1):\n",
    "                    k = np.nonzero((data[:,x[j]]-data[:,y[j]])==0)[0][0]\n",
    "                    Summ += '. All except for '+str(xlabs[k])+t+' \\''+str(ylabels[y[j]])+'\\' is equal to \\''+str(ylabels[x[j]])+'\\''\n",
    "\n",
    "\n",
    "    Summ = GrammerCorrect(Summ+'.')\n",
    "    text_file = open(path+\"FinalSummary_\"+str(imgno)+\".txt\", \"w\")\n",
    "    n = text_file.write(Summ)\n",
    "    text_file.close()\n",
    "    print(imgno,\"\\n\",Summ)\n",
    "    print(\"________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'lowest', 'd', 'whereas', 'e', 'five', 'chart', 'of', 'describes', 'temperature', 'is', '4-10', 'coldest', 'which', 'the', 'has', 'winters', 'with', 'c', 'about', 'cities', 'city', 'degree', 'ranges', 'among', 'chills', 'highest', '483', 'and', 'celsius', 'experiences', 'summer', 'for', '3968', 'during', 'relation', 'between', '2099', 'in']\n",
      "['from', 'having', 'values', 'x-axis', 'd', 'e', 'of', 'temperature', 'is', 'range', 'winter', 'the', '3944', 'c', 'with', 'grouped', '(*c)', 'city', 'y-axis', '168', 'and', '2092', 'plot', 'summer', 'deviation', 'graph', 'b', 'for', 'depicts', 'a', 'illustrating', '631', 'standard', 'list', 'bar', 'vertical', '925', '484', 'to', 'on']\n",
      "['', 'at', 'having', 'till', 'values', 'd', '16', 'labels', 'e', 'of', '2444', 'temperature', 'difference', 'is', 'minimum', 'winter', '69', 'respectively', 'the', '3944', 'c', 'with', '12', 'grouped', '(*c)', 'city', 'y-axis', '876', 'categories', 'maximum', 'starts', '168', 'and', 'ends', '2092', 'finally', 'plot', 'summer', 'deviation', 'graph', 'b', 'for', 'depicts', 'a', '582', 'are', 'then', '2778', 'increases', 'illustrating', 'value', '631', 'standard', 'between', 'bar', 'vertical', '925', '484', 'in', '2562', 'on']\n",
      "3.549225288188377e-155 9.74806386640271e-232 39 40 61\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# ### hb3\n",
    "# ref = 'The chart image describes the relation between abortion rate and different age groups. The age groups are shown are in the range of 10-45 with interval of 5. The highest abortion rate is shown by tallest bar representing the age group of 20.'\n",
    "# final = 'The plot depicts a Horizontal Simple Bar Graph illustrating Abortion Rate among different age groups.  The plot is between Age on y-axis over Abortion rate on the x-axis.  In the Age ranging form 15-40 at the interval 5, the Abortion rate are 22.8, 35.4, 24.2, 16.07, 11.07, and 3.73 respectively. The overall mean and standard deviation values of Abortion rate are 18.88 and 10.12 respectively.'\n",
    "# intial = 'The plot depicts a Horizontal Simple Bar Graph illustrating Abortion Rate among different age groups. The plot is between Age on y-axis over Abortion rate on the x-axis. The Abortion rate starts with 22 in 14 then increases till 19, followed by a decreasing trend till 39 the end'\n",
    "\n",
    "### gb03\n",
    "ref = 'The chart image describes about the relation between the highest temperature in summer and lowest temperature in winters for five cities. During summer, the temperature is highest in City D with 39.68 degree Celsius whereas city E has lowest temperature of 20.99 degree Celsius. During winters, the temperature ranges between 4-10 degree Celsius for the five cities among which city C experiences the coldest winters with the chills of 4.83 degree Celsius.'\n",
    "final = 'The plot depicts a Vertical Grouped Bar Graph illustrating City Temperature.  The plot is having Temperature (*C) on y-axis for summer, and winter.  The Temperature (*C) of summer are 25.62, 24.44, 27.78, 39.44, and 20.92 for the labels City A, City B, City C, City D, and City E respectively.  The Temperature (*C) of winter are 9.25, 5.82, 4.84, 6.9, and 8.76 for the labels City A, City B, City C, City D, and City E respectively. The Temperature (*C) difference between summer and winter starts with 16 at City at then increases till City D the maximum value, and finally ends with 12 in City E the minimum value.  The standard deviation values of Temperature (*C) for categories summer, and winter are 6.31, and 1.68 respectively.'\n",
    "intial = 'The plot depicts a Vertical Grouped Bar Graph illustrating City Temperature. The plot is having Temperature (*C) on y-axis for summer, and winter. The list of X-axis values is City A, City B, City C, City D, and City E. The summer range from 20.92 to 39.44, with a standard deviation of 6.31. The winter range from 4.84 to 9.25, with a standard deviation of 1.68'\n",
    "\n",
    "reference = ref.replace(\",\", \"\").replace('.', '').split(' ')\n",
    "reference = list(set(map(lambda x: x.lower(), reference)))\n",
    "candidatei = intial.replace(\",\", \"\").replace('.', '').split(' ')\n",
    "candidatei = list(set(map(lambda x: x.lower(), candidatei)))\n",
    "candidatef = final.replace(\",\", \"\").replace('.', '').split(' ')\n",
    "candidatef = list(set(map(lambda x: x.lower(), candidatef)))\n",
    "\n",
    "print(reference)\n",
    "print((candidatei))\n",
    "print(candidatef)\n",
    "scorei = sentence_bleu(reference, candidatei)\n",
    "scoref = sentence_bleu(reference, candidatef)\n",
    "print(len(reference), len(candidatei), len(candidatef),scorei,scoref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
